{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Questa tecnica serve a trovare la policy in una situazione in cui il modello(MDP) è sconosciuto. Anche qui sorge il problema della **Exploration VS Exploitation**\n",
    "\n",
    "\n",
    "### Reinforcement learning passivo\n",
    "Si assume che una policy $\\pi$ sia data e fissata, l'agente deve imparare quanto è buona la policy.\n",
    "\n",
    "In sostanza bisogna imparare $V^{\\pi}(s)$ dove s è un qualsiasi stato raggiungibile eseguendo $\\pi$ un numero finito di volte.\n",
    "\n",
    "Soluzione: **Adaptive Dynamic Programming**\n",
    "Genera un Dataset che contiene diversi episodi, ovvero una sequenza di transizioni e reward.\n",
    "Stimo l'MDP e valuto la bontà della policy sulla stima.\n",
    "Questo metoo è detto model-based\n",
    "\n",
    "Approccio alternativo **Model-Free**, stimo direttamente i valore della state-value function.\n",
    "\n",
    "#### Temporal Difference Learning\n",
    "$$\\hat{V}^{\\pi}(s) \\leftarrow \\hat{V}^{\\pi}(s) + \\alpha \\left( z - \\hat{V}^{\\pi}(s)\\right)$$\n",
    "\n",
    "Ad ogni iterazione modifico il valore di V secondo $\\alpha$ ovvero l'indice di apprendimento, maggiore è $\\alpha$ maggiore sarà la tendenza di V ad adattarsi a z(Il valore sperimentato).\n",
    "\n",
    "Funzionamento algoritmo:\n",
    "1. Inizializzo $\\hat{V}^\\pi (s_0) = 0$ per lo stato iniziale e inizio l'algoritmo con $s \\leftarrow s_0$\n",
    "2. Osservo lo stato corrente $s$ ed eseguo $\\pi(s)$ e osserva il reward $R$ e lo stato di arrivo $s'$, se è la prima volta che sono in $s'$ inizializzo $\\hat{V}^\\pi (s') = 0$ \n",
    "3. Calcolo $z = R + \\gamma \\hat{V}^\\pi (s')$\n",
    "4. aggiorno la valutazione di s: $\\hat{V}^{\\pi}(s) \\leftarrow \\hat{V}^{\\pi}(s) + \\alpha \\left( z - \\hat{V}^{\\pi}(s)\\right)$\n",
    "5. Riparti da 2 con $s \\leftarrow s'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning attivo\n",
    "\n",
    "La policy non è data, l'agente la deve calcolare.\n",
    "\n",
    "**Active ADP naive**: uso una policy fissa per esplorare l'ambiente. (Non funziona).ù\n",
    "\n",
    "Un altro approccio è quello di esplorare in modo totalmente randomico, manca di Exploitation.\n",
    "\n",
    "Un approccio migliore è quello di attribuire una policy arbitraria, stimare l'ambiente e poi stabilire una nuova stima della policy ottima, una volta fatto questo ripetere la stima con la nuova policy (**Greedy**). Questo approccio manca di Exploration. Possiamo migliore questo approccio aggiungendo un valore $\\epsilon$ che determina la probabilità di svolgere un azione randomica invece di seguire la policy ($\\epsilon$**-greedy**). Inoltre si èuò ridurre $\\epsilon$ nel tempo per ridurre la variazione della policy ottima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
