{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Search\n",
    "\n",
    "La caratteristica principale di questo tipo di problemi è la presenza di altri agenti. Sono anche chiamati giochi.\n",
    "\n",
    "Caratteristiche:\n",
    "- Ogni agente ha preerenze diverse\n",
    "- Le azioni di un agente influenzano l'ambiente e quindi gli altri agenti\n",
    "- L'agente deve considerare le strategie degli altri decisori.\n",
    "\n",
    "\n",
    "Classificazione dei giochi:\n",
    "- 2 o n giocatori\n",
    "- agenti razionali, $\\epsilon$-razionali?\n",
    "- Struttura sequenziale: turni o azioni simultanee\n",
    "- Deterministico o stocastico\n",
    "- Struttura dei payoff: somma costante o somma generica.\n",
    "- informazione completa o incompleta\n",
    "- informazione perfetta o imperfetta: i giocatori sono informati di tutto ciò che è successo.\n",
    "\n",
    "Teoria dei giochi competitiva: studio del comportamento individuale dei singoli agenti.\n",
    "Teoria dei giochi cooperativa: studio della formzione di coalizioni.\n",
    "\n",
    "Formalizziamo i giochi (Meccanismo di gioco):\n",
    "- Stati: $S = \\{s_1, s_2, \\dots\\}$\n",
    "- Giocatori e zioni possibili: insieme degli agenti $I = \\{i_1, i_2\\}$ insieme della azioni possibili $A=\\{a_1, a_2, a_3, \\dots\\}$\n",
    "- Turni e azioni legali: $I(s_k) \\in I$è il giocatore che può agire nello stato $s_k$ e $A(s_k)$ sono le azioni che può intraprendere\n",
    "- Modello di transzioni\n",
    "- Stati terminali: dove il gioco termina e viene dato un payoff\n",
    "- Utilità: payoff che il giocatore riceve in quello stato\n",
    "\n",
    "Altre classificazioni:\n",
    "- Informazione completa: ogni agente ha accesso al meccanismo\n",
    "- Informazione perfetta: ogni agente ha accesso alla sequenza di azioni avvenute.\n",
    "- A somma zero: un guadagno equivale una perdita\n",
    "\n",
    "Oltre il meccanismo il gioco e composto dalle strategie. La strategia specifica il comportamento di un giocatore in un determinato stato, può essere:\n",
    "- Pura: ad una strategia corrisponde una singola azione.\n",
    "- mista: ad una strategia corrisponde uno spazio di probabilità di azioni.\n",
    "\n",
    "Strategia ottima: la migliore strategia conotro un avversario infallibile.\n",
    "\n",
    "Risolvere un gioco: calcolare la strategia migliore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimax\n",
    "\n",
    "I giocatori sono MAX(quello che deve massimizzare) e MIN(con obbiettivo di minimizzare).\n",
    "\n",
    "MAX avrà sempre utilità $\\geq 0$ mentre MIN $< 0$\n",
    "\n",
    "La strategia Min-Max è quella migliore considerando un giocatore razionale, in caso di avversario non ottimo la strategia Min-Max non peggiora il risultato.\n",
    "\n",
    "MinMax può essere usato anche con più agenti, in questo caso non esiste più il concetto di massimizzatore e minimizzatore, ogni agente avrà la propria utilità privata e dovrà provare a massimizzarla.\n",
    "\n",
    "Min-Max è estremamente inefficiente, occorrono tecniche di Pruning.\n",
    "\n",
    "# $\\alpha - \\beta$ pruning\n",
    "\n",
    "Teniamo traccia di quello che abbiamo scoperto di un nodo.\n",
    "\n",
    "Es: se da un nodo MAX scopriamo un figlio con valore x, il valore di v è almeno x\n",
    "Es: se da un nodo MIN scopriamo un figlio con valore x, il valore di v è al più x\n",
    "\n",
    "ad ogni nodo assaciamo due valori $\\alpha$ e $\\beta$ che sono rispettivamente il minimo valore garantito per MAX e il massimo valore garantito per MIN.\n",
    "\n",
    "Questo algoritmo dipende dall'ordinamento dei nodi, l'ordinamento peggiore rende intile il pruning con un una complessità di $O(b^d)$ (Branchi factor e Depth) mentre con l'ordinamento migliore la complessità diventa $O(b^{\\frac{d}{2}})$\n",
    "\n",
    "\n",
    "Questa tecnica spesso risulta impossibile in molti casi reali per motivi di tempo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione di valutazione e cutoff\n",
    "\n",
    "Invece di calcolare il valore MinMax si utilizza una funzione di Valutazione V(s) si fornisce una stima di quel valore. V è di fatto un'euristica e deve essere efficiente.\n",
    "\n",
    "Data v e un test di cutoff CUT(s,d) se il test dice false eseguo minmax, altrimenti fermo minmax e restituisco v(s)\n",
    "\n",
    "Valutazione per feature: è possbile trovare delle \"Caratteristiche\" che rendono gli stati equivalenti tra loro.\n",
    "\n",
    "Funzione basata sull'esperienza: costruiamo un dataset in base alle precedenti partite, da quello stabiliamo con l'attuale vettore di feature la valutazione dello stato corrente.\n",
    "\n",
    "Funzione di Cutoff:\n",
    "- Basata su su soglia di profondità massima\n",
    "- Identificazione degli stati quiescenti: ovvero gli stati in cui non ci si aspetta tanto cambiamento.\n",
    "- Trasposizioni: due stati potrebbero essere strategicamente equivalenti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pruning**\n",
    "\n",
    "Ad ogni livello possiamo scegliere solo un numero $w$ di azioni, possiamo scegliere qualce scartare attraverso un'euristica. Il nuovo algoritmo diventa il **Beam search**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giochi stocastici\n",
    "\n",
    "Si introduce un nuovo giocatore la natura: $N$, i suoi nodi sono detti **Nodi di Chance**. Caratteristiche:\n",
    "- La sua strategia è fissata a priori e conosciuta da tutti i giocatori. \n",
    "- La sua strategia di gioco è mista, ovvero è una distribuzione di probabilità.\n",
    "- Non riceve un payoff, quindi la sua strategia è un fenomeno dato.\n",
    "\n",
    "**Expectimax**: potatura $\\alpha$-$\\beta$ ma basato sul valore atteso.\n",
    "\n",
    "**Monte Carlo Tree Search**: simulare alto numero di partite random, la media dei punteggi finali p la stima del valore degli stati, per ogni azione scelgo quella con la media più alta.\n",
    "\n",
    "Dilemma di **Exploration VS Exploitation**: bisogna stare attenti a non scartare azioni che non sembrano buone e sfruttare troppo le azioni che sembrano buone.\n",
    "\n",
    "Soluzione ottima: simula l'azione che massimizza l'Upper Confidence Bound: $UCB_i = w_i + c \\sqrt{\\frac{log N}{N_i}}$ dove $N_i$ è il numero di simulazioni svolte per $a_i$ e $c$ è un parametro che bilancia exploration e exploitation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
